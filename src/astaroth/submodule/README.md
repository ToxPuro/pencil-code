# Astaroth - A Scalable Multi-GPU Library for Stencil Computations {#mainpage}

[API specification](doc/Astaroth_API_specification_and_user_manual/API_specification_and_user_manual.md)| [Astaroth DSL](acc-runtime/README.md) | [Contributing](CONTRIBUTING.md) | [Licence](LICENCE.md) | [Repository](https://bitbucket.org/jpekkila/astaroth) | [Issue Tracker](https://bitbucket.org/jpekkila/astaroth/issues?status=new&status=open) | [Wiki](https://bitbucket.org/jpekkila/astaroth/wiki/Home)

Astaroth is a multi-GPU library designed for high-order stencil computations with a special focus on the requirements of multiphysics applications on modern HPC systems. It provides a multi-GPU and single-GPU APIs, a domain-specific language for expressing stencil codes in a high-level syntax, and an optimizing compiler that translates DSL sources into CUDA/HIP subroutines that exhibit near handtuned performance.

Astaroth is licenced under the terms of the GNU General Public Licence, version 3, or later
(see [LICENCE.txt](LICENCE.md)). For contributing guidelines,
see [Contributing](CONTRIBUTING.md).


## System Requirements
* An NVIDIA GPU with support for compute capability 3.0 or higher (Kepler architecture or newer)

Or

* an AMD GPU that has HIP support

## Dependencies
Relative recent versions of

`gcc cmake flex bison`

and either `nvcc` (NVIDIA) or `hipcc` (AMD). The library is built for NVIDIA by default, but AMD support can be enabled by passing `-DUSE_HIP=ON` to CMake.

## Choosing a branch

`master` is a tested, stable branch. We recommend to use this branch in production

`develop` is the main development branch.

## Building

In the base directory, run

1. `mkdir build`
2. `cd build`
3. `cmake ..`
4. `make -j`

> **Optional:** Documentation can be generated by running `doxygen` in the base directory. Generated documentation can be found in `doc/doxygen`.

> **Tip:**  The library is configured by passing [options](#markdown-header-cmake-options) to CMake with `-D[option]=[ON|OFF]`. For example, double precision can be enabled by calling `cmake -DBUILD_DOUBLE_PRECISION=ON ..`. See [CMakeLists.txt](https://bitbucket.org/jpekkila/astaroth/src/master/CMakeLists.txt) for an up-to-date list of options.

> **Note:** CMake will inform you if there are missing dependencies.

## Simple example template

If you need a functioning and documented example of how to run Astaroth in standalone more out of the box,

[please see this shock turbulence run example](config/samples/shockturb/README.md) in the directory `config/samples/shockturb/`. 

## Running on clusters (Slurm)

1. Load the modules required for building: `module load gcc/8.3.0 cuda/10.1.168 cmake openmpi/4.0.3-cuda`
1. Build with MPI support: `cmake -DMPI_ENABLED=ON ..` (see 'Building' above)
1. Run the code interactively or with a batch job, f.ex. `srun --account=<project number> --gres=gpu:v100:4 --mem=24000 -t 00:14:59 -p gputest --ntasks-per-socket=2 -n 4 -N 1 <executable here, f.ex ./mpitest or ./benchmark 256 256 256 or ./ac_run -t>`

> **Note:** Assign one task per GPU when using MPI.

> **Tip:** For more information on running the code on Puhti, see [Puhti docs](https://docs.csc.fi/computing/overview/).

### MPI

Getting the modules and slurm commands right can be tricky because they vary system by system. See a list of systems and commands that have been tested to work below.

```
# CSC Puhti (2023-02-28)
module load gcc cuda openmpi cmake

Currently Loaded Modules:
  1) csc-tools (S)   2) StdEnv   3) gcc/11.3.0   4) intel-oneapi-mkl/2022.1.0   5) cuda/11.7.0   6) openmpi/4.1.4   7) cmake/3.23.1

srun --account=$PROJECT --gres=gpu:v100:4 --mem=24000 -t 00:14:59 -p gputest --ntasks-per-socket=2 --ntasks-per-node=4 -N 2
```

```
# CSC Mahti (2023-01-24)
module load gcc/9.4.0 openmpi/4.1.2-cuda cuda cmake

Currently Loaded Modules:
  1) csc-tools (S)   3) gcc/9.4.0             5) openmpi/4.1.2-cuda   7) cmake/3.21.4
  2) StdEnv          4) openblas/0.3.18-omp   6) cuda/11.5.0

srun --account=$PROJECT --gpus-per-node=4 --mem=24000 -t 00:14:59 -p gpumedium --ntasks-per-node=4 -N 2
```

```
# CSC Lumi (2023-01-24)
# NOTE! System is still being installed, modules and settings highly likely to change.
module purge
module load CrayEnv
module load PrgEnv-cray
module load craype-accel-amd-gfx90a
module load rocm
module load buildtools
module load cray-python

Currently Loaded Modules:
  1) init-lumi/0.1                        (S)   7) CrayEnv               (S)  13) PrgEnv-cray/8.3.3
  2) ModuleLabel/label                    (S)   8) cce/14.0.2                 14) craype-accel-amd-gfx90a
  3) craype-x86-rome                            9) craype/2.7.17              15) rocm/5.0.2
  4) libfabric/1.15.0.0                        10) cray-dsmml/0.2.2           16) buildtools/22.08
  5) craype-network-ofi                        11) cray-mpich/8.1.18          17) cray-python/3.9.12.1
  6) xpmem/2.4.4-2.3_9.1__gff0e1d9.shasta      12) cray-libsci/22.08.1.1

SRUNMPI8="srun --account=$PROJECT -t 00:05:00 -p dev-g --gpus-per-node=8 --ntasks-per-node=8 --nodes=1"
SRUNMPI16="srun --account=$PROJECT -t 00:05:00 -p dev-g --gpus-per-node=8 --ntasks-per-node=8 --nodes=2"
```

```
# Aalto Triton (2023-01-24)
git clone git@bitbucket.org:jpekkila/astaroth.git
cd astaroth && checkout develop             # Needed for up-to-date AMD support
mkdir build && cd build                     # Create a build directory

module load gcc bison flex cmake openmpi
srun -p gpu-amd -t 00:05:00 --pty /bin/bash # Currently need to build on the gpu-amd partition, otherwise hipcc is not available
cmake -DBUILD_SHARED_LIBS=ON .. && make -j  # Hangs with the hip compiler on Triton if BUILD_SHARED_LIBS=OFF due linker failure
./devicetest                                # Check that everything works
```


## CMake Options

| Option | Description | Default |
|--------|-------------|---------|
| CMAKE_BUILD_TYPE | Selects the build type. Possible values: Debug, Release, RelWithDebInfo, MinSizeRel. See (CMake documentation)[https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html] for more details. | Release |
| CUDA_ARCHITECTURES | Selects CUDA architecture support. Multiple architectures delimited by `;`. See (CMake documentation)[https://cmake.org/cmake/help/latest/prop_tgt/CUDA_ARCHITECTURES.html] for more details. | "60;70" |
| DOUBLE_PRECISION | Generates double precision code. | ON |
| BUILD_SAMPLES | Builds projects in samples subdirectory. | ON |
| MPI_ENABLED | Enables acGrid functions for carrying out computations with MPI. | OFF |
| USE_CUDA_AWARE_MPI | Uses GPUDirect RDMA for direct GPU-GPU communication instead of routing communication through host memory | ON |
| MULTIGPU_ENABLED | Enables Astaroth to use multiple GPUs on a single node. Uses peer-to-peer communication instead of MPI. Affects Legacy & Node layers only. | ON |
| DSL_MODULE_DIR | Defines the directory to be scanned when looking for DSL files. | `acc-runtime/samples/mhd_modular` |
| PROGRAM_MODULE_DIR | Can be used to declare additional host-side program modules (also known as Thrones) | empty |
| VERBOSE | Enables various non-critical warning and status messages. | OFF |
| BUILD_UTILS | "Builds the utility library. | ON |
| BUILD_ACC_RUNTIME_LIBRARY | "Builds the standalone acc runtime library" | OFF |
| USE_HIP | "Use HIP as the underlying GPGPU library instead of CUDA" | OFF |
| SINGLEPASS_INTEGRATION| "Perform integration in a single pass. Improves performance by ~20% but may introduce slightly larger floating-point arithmetic error than the conventional approach" | ON |
| OPTIMIZE_MEM_ACCESSES | "Optimizes memory accesses by computing only the bare minimum number of stencils but can introduce errors in some use cases, f.ex. if a stencil is accessed conditionally based on a value not known at compile time" | OFF |
| BUILD_SHARED_LIBS | "Builds Astaroth as a collection of shared libraries instead of statically built modules" | OFF |


## Standalone Module


```Bash
Usage: ./ac_run [options]
	     --help | -h: Prints this help.
	     --test | -t: Runs autotests.
	--benchmark | -b: Runs benchmarks.
	 --simulate | -s: Runs the simulation.
	   --render | -r: Runs the real-time renderer.
	   --config | -c: Uses the config file given after this flag instead of the default.
```

See `analysis/python/` directory of existing data visualization and analysis scripts.

## Interface

* `astaroth/include/astaroth.h`: Astaroth main header. Contains the interface for accessing single- and multi-GPU layers.
* `astaroth/include/astaroth_utils.h`: Utility library header. Provides functions for performing common tasks on host, such as allocating and verifying meshes.
* `acc-runtime/api/acc_runtime.h`: Low-level single-GPU library for running kernels generated from user-defined DSL files.
* ~~`<build directory>/astaroth.f90`: Fortran interface to Astaroth. Generated when building the library.~~ (not available at the moment)

## FAQ

Can I use the code even if I don't make my changes public?

> [GPL](LICENCE.md) requires only that if you release a binary based on Astaroth to the public, then you should also release the source code for it. In private you can do whatever you want (secret forks, secret collaborations, etc). **Astaroth Code source files (.ac, .h) do not belong to the library and therefore are not licenced under GPL.** The user who created the files holds copyright over them and can choose to distribute them under any licence.

How do I compile with MPI support?

> Ensure that your MPI implementation has been built with CUDA support and invoke CMake with `cmake -DMPI_ENABLED=ON -DBUILD_SAMPLES=ON ..`. Otherwise the build steps are the same. Assign exactly one process per GPU and run with, for example, `srun --gres=gpu:v100:<ngpus per node> --ntasks-per-socket=<ngpus per node / NICs per node> -n <total number of gpus> -N <number of nodes> ./mpitest` or for `mpirun` the command `mpirun -np <ngpus per node>  ./mpitest` works (at least on a GPU node).

I have issues with MPI

> If your MPI has been setup incorrectly or does not support CUDA-aware communication, you can try building Astaroth without RDMA support with `cmake -DUSE_CUDA_AWARE_MPI=OFF ..`. Note that without CUDA-aware support, communication is routed through CPU memory which gives notably worse performance than communicating directly between GPUs.

I have issues with IBM Power PCs

> This may be due to `utils` library requiring vectorization. You can try replacing `-mavx` with `-maltivec` or `-mcpu=native` in `src/utils/CMakeLists.txt`. Otherwise, the `core` library does not use vectorization, so using Astaroth without the `utils` component should be possible.

How do I contribute?

> See [Contributing](CONTRIBUTING.md).
